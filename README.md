# Multi-Modal-Machine-Learning-Resources

## Journal

*IEEE Transactions on Multimedia (TMM)*<br>

*IEEE Multimedia Magazine (MM)*<br>

*Information Fusion (IF)*<br>

## Coference

*ACM International Conference on Multimedia (ACM MM)*<br>

*IEEE International Conference on Multimedia & Expo (ICME)*<br>

## Group

**Multimodal Communication and Machine Learning Laboratory (MultiComp Lab)**<br>
*Louis-Philippe Morency*<br>
*Carnegie Mellon University*<br>
[[Homepage](http://multicomp.cs.cmu.edu/)]

## Resource

**awesome-multimodal-ml**<br>
[[Github](https://github.com/pliang279/awesome-multimodal-ml)]

**Awesome-Multimodal-Research**<br>
[[Github](https://github.com/Eurus-Holmes/Awesome-Multimodal-Research)]

## Review

**Multimodal Machine Learning A Survey and Taxonomy.**<br>
*Tadas Baltrušaitis; Chaitanya Ahuja; Louis-Philippe Morency.*<br>
TPAMI, 2019. [[PDF](https://arxiv.org/pdf/1705.09406)]

**Deep multimodal learning A survey on recent advances and trends.**<br>
*D Ramachandram, GW Taylor.*<br>
TIP, 2017. [[PDF](https://ieeexplore.ieee.org/abstract/document/8103116)]

**Multimodal intelligence: Representation learning, information fusion, and applications.**<br>
*C Zhang, Z Yang, X He, L Deng.*<br>
IEEE Journal of Selected Topics in Signal Processing, 2020. [[PDF](https://arxiv.org/pdf/1911.03977)]

**Multibench: Multiscale benchmarks for multimodal representation learning.**<br>
*PP Liang, Y Lyu, X Fan, Z Wu, Y Cheng, J Wu, et al.*<br>
NeurlPS, 2021. 

## Representation

**DeViSE: A Deep Visual-Semantic Embedding Model.**<br>
*A Frome, G Corrado, J Shlens, S Bengio, J Dean, et al.*<br>
NIPS, 2013. [[PDF](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.466.176&rep=rep1&type=pdf)]

**Learning joint embedding with multimodal cues for cross-modal video-text retrieval.**<br>
*NC Mithun, J Li, F Metze, et al.*<br>
ICMR, 2018. [[PDF](https://dl.acm.org/doi/pdf/10.1145/3206025.3206064)]

**Deep multimodal representation learning: A survey.**<br>
*W Guo, J Wang, S Wang.*<br>
IEEE Access, 2019. [[PDF](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8715409)]

## Translation

**Show and tell: A neural image caption generator.**<br>
*O Vinyals, A Toshev, S Bengio, et al.*<br>
CVPR, 2015. [[PDF](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Vinyals_Show_and_Tell_2015_CVPR_paper.pdf)]

**Show, attend and tell: Neural image caption generation with visual attention.**<br>
*K Xu, J Ba, R Kiros, K Cho, A Courville, et al.*<br>
PMLR, 2018. [[PDF](https://www.researchgate.net/publication/272194766_Show_Attend_and_Tell_Neural_Image_Caption_Generation_with_Visual_Attention)]

**A survey on automatic image caption generation.**<br>
*S Bai, S An.*<br>
Neurocomputing, 2018. [[PDF](http://press.liacs.nl/students.mir/inspiration/A%20survey%20on%20automatic%20image%20caption%20generation.Neurocomputing2018.pdf)]

## Alignment

**Stacked cross attention for image-text matching.**<br>
*KH Lee, X Chen, G Hua, H Hu, et al.*<br>
CVPR, 2018. [[PDF](https://openaccess.thecvf.com/content_ECCV_2018/papers/Kuang-Huei_Lee_Stacked_Cross_Attention_ECCV_2018_paper.pdf)]

**Visual Semantic Reasoning for Image-Text Matching.**<br>
*K Li, Y Zhang, K Li, Y Li, Y Fu.*<br>
CVPR, 2020. [[PDF](https://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Visual_Semantic_Reasoning_for_Image-Text_Matching_ICCV_2019_paper.pdf)]

**Oscar Object-Semantics Aligned Pre-training for Vision-Language Tasks.**<br>
*X Li, X Yin, C Li, P Zhang, X Hu, L Zhang, et al.*<br>
ECCV, 2020. [[PDF](https://arxiv.org/pdf/2004.06165)]

**Fashionbert: Text and image matching with adaptive loss for cross-modal retrieval.**<br>
*D Gao, L Jin, B Chen, M Qiu, P Li, Y Wei, Y Hu, et al.*<br>
SIGIR, 2020. [[PDF](https://dl.acm.org/doi/pdf/10.1145/3397271.3401430)]

**Similarity Reasoning and Filtration for Image-Text Matching.**<br>
*H Diao, Y Zhang, L Ma, H Lu.*<br>
AAAI, 2021. [[PDF](https://arxiv.org/pdf/2101.01368.pdf)]

## Fusion

**Attention-Based Multimodal Fusion for Video Description.**<br>
*C Hori, T Hori, TY Lee, Z Zhang, et al.*<br>
CVPR, 2017. [[PDF](https://openaccess.thecvf.com/content_ICCV_2017/papers/Hori_Attention-Based_Multimodal_Fusion_ICCV_2017_paper.pdf)]

**Multimodal keyless attention fusion for video classification.**<br>
*X Long, C Gan, G De Melo, X Liu, Y Li, F Li, et al.*<br>
AAAI, 2018. [[PDF](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/17054/16313)]

**MFAS Multimodal Fusion Architecture Search.**<br>
*JM Pérez-Rúa, V Vielzeuf, S Pateux, et al.*<br>
CVPR, 2019. [[PDF](https://openaccess.thecvf.com/content_CVPR_2019/papers/Perez-Rua_MFAS_Multimodal_Fusion_Architecture_Search_CVPR_2019_paper.pdf)]

**Quantum-inspired multimodal fusion for video sentiment analysis.**<br>
*Q Li, D Gkoumas, C Lioma, M Melucci.*<br>
Information Fusion, 2020. [[PDF](https://arxiv.org/pdf/2103.10572.pdf)]

**A survey on machine learning for data fusion.**<br>
*T Meng, X Jing, Z Yan, W Pedrycz.*<br>
Information Fusion, 2020. [[PDF](https://www.researchgate.net/publication/337865553_A_Survey_on_Machine_Learning_for_Data_Fusion)]

**Attention bottlenecks for multimodal fusion.**<br>
*A Nagrani, S Yang, A Arnab, A Jansen, et al.*<br>
NIPS, 2021. [[PDF](https://proceedings.neurips.cc/paper/2021/file/76ba9f564ebbc35b1014ac498fafadd0-Paper.pdf)]

**Dynamic Multimodal Fusion.**<br>
*Z Xue, R Marculescu.*<br>
ArXiv, 2022. [[PDF](https://arxiv.org/pdf/2204.00102.pdf)]

## Co-learning

**Visualbert: A simple and performant baseline for vision and language.**<br>
*LH Li, M Yatskar, D Yin, CJ Hsieh, et al.*<br>
ArXiv, 2019. 

**Lxmert: Learning cross-modality encoder representations from transformers.**<br>
*H Tan, M Bansal.*<br>
EMNLP, 2019. [[PDF](https://arxiv.org/pdf/1908.07490.pdf)]

**Videobert: A joint model for video and language representation learning.**<br>
*C Sun, A Myers, C Vondrick, et al.*<br>
ICCV, 2019. 

**Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks.**<br>
*J Lu, D Batra, D Parikh, S Lee.*<br>
NIPS, 2019. [[PDF](https://arxiv.org/pdf/1908.02265.pdf%20http://arxiv.org/abs/1908.02265.pdf)]

**Univl: A unified video and language pre-training model for multimodal understanding and generation.**<br>
*H Luo, L Ji, B Shi, H Huang, N Duan, T Li, J Li, et al.*<br>
ArXiv, 2020. [[PDF](https://arxiv.org/pdf/2002.06353.pdf)]

**Actbert: Learning global-local video-text representations.**<br>
*L Zhu, Y Yang.*<br>
CVPR, 2020. 

**What Makes Training Multi-modal Classification Networks Hard.**<br>
*W Wang, D Tran, M Feiszli.*<br>
CVPR, 2020. [[PDF](https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_What_Makes_Training_Multi-Modal_Classification_Networks_Hard_CVPR_2020_paper.pdf)]

**Hero: Hierarchical encoder for video+ language omni-representation pre-training.**<br>
*L Li, YC Chen, Y Cheng, Z Gan, L Yu, J Liu.*<br>
EMNLP, 2020. 

**Vl-bert: Pre-training of generic visual-linguistic representations.**<br>
*W Su, X Zhu, Y Cao, B Li, L Lu, F Wei, J Dai.*<br>
ICLR, 2020. 

**Foundations of multimodal co-learning.**<br>
*A Zadeh, PP Liang, LP Morency.*<br>
Information Fusion, 2020. [[PDF](https://www.sciencedirect.com/science/article/pii/S1566253520303006)]

**Unimo: Towards unified-modal understanding and generation via cross-modal contrastive learning.**<br>
*W Li, C Gao, G Niu, X Xiao, H Liu, J Liu, H Wu, et al.*<br>
ArXiv, 2021. [[PDF](https://arxiv.org/pdf/2012.15409.pdf)]

**M6: A chinese multimodal pretrainer.**<br>
*J Lin, R Men, A Yang, C Zhou, M Ding, Y Zhang, et al.*<br>
ArXiv, 2021.

**Multimodal Co-learning: Challenges, Applications with Datasets, Recent Advances and Future Directions.**<br>
*A Rahate, R Walambe, S Ramanna, et al.*<br>
ArXiv, 2021. [[PDF](https://arxiv.org/ftp/arxiv/papers/2107/2107.13782.pdf)]

**Towards a Unified Foundation Model: Jointly Pre-Training Transformers on Unpaired Images and Text.**<br>
*Q Li, B Gong, Y Cui, D Kondratyuk, X Du, et al.*<br>
ArXiv, 2021.

**VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts.**<br>
*W Wang, H Bao, L Dong, F Wei.*<br>
ArXiv, 2021.

**WenLan: Bridging vision and language by large-scale multi-modal pre-training.**<br>
*Y Huo, M Zhang, G Liu, H Lu, Y Gao, G Yang, et al.*<br>
ArXiv, 2021.

**WenLan 2.0: Make AI Imagine via a Multimodal Foundation Model.**<br>
*N Fei, Z Lu, Y Gao, G Yang, Y Huo, J Wen, H Lu, R Song, X Gao, T Xiang, et al.*<br>
ArXiv, 2021.

**Learning transferable visual models from natural language supervision.**<br>
*A Radford, JW Kim, C Hallacy, et al.*<br>
ICML, 2021.

**Vilt: Vision-and-language transformer without convolution or region supervision.**<br>
*W Kim, B Son, I Kim.*<br>
ICML, 2021.

**Zero-shot text-to-image generation.**<br>
*A Ramesh, M Pavlov, G Goh, S Gray, et al.*<br>
ICML, 2021.

**Multimodal few-shot learning with frozen language models.**<br>
*M Tsimpoukelli, J Menick, S Cabi, et al.*<br>
NIPS, 2021.

**Wukong: 100 Million Large-scale Chinese Cross-modal Pre-training Dataset and A Foundation Framework.**<br>
*J Gu, X Meng, G Lu, L Hou, M Niu, H Xu, et al.*<br>
ArXiv, 2022.

## Applications

### Understanding

#### Image

**Deep Collaborative Embedding for Social Image Understanding.**<br>
*Z Li, J Tang, T Mei.*<br>
TPAMI, 2019. 
[[PDF](https://ieeexplore.ieee.org/document/8403294)]

**Layoutlm: Pre-training of text and layout for document image understanding.**<br>
*Y Xu, M Li, L Cui, S Huang, F Wei, M Zhou.*<br>
KDD, 2020. 

**Beyond visual semantics: Exploring the role of scene text in image understanding.**<br>
*AU Dey, SK Ghosh, E Valveny, G Harit.*<br>
Pattern Recognition Letters, 2021. 

#### Video 

**Eco Efficient convolutional network for online video understanding.**<br>
*M Zolfaghari, K Singh, T Brox.*<br>
ECCV, 2018. 
[[PDF](https://openaccess.thecvf.com/content_ECCV_2018/papers/Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper.pdf)]

**TSM Temporal Shift Module for Efficient Video Understanding.**<br>
*J Lin, C Gan, S Han.*<br>
CVPR, 2019. 
[[PDF](https://openaccess.thecvf.com/content_ICCV_2019/papers/Lin_TSM_Temporal_Shift_Module_for_Efficient_Video_Understanding_ICCV_2019_paper.pdf)]

**Assemblenet: Searching for multi-stream neural connectivity in video architectures.**<br>
*MS Ryoo, AJ Piergiovanni, M Tan, et al.*<br>
ICLR, 2020. 

**Is Space-Time Attention All You Need for Video Understanding?**<br>
*G Bertasius, H Wang, et al.*<br>
ArXiv, 2021. 

**MDMMT Multidomain Multimodal Transformer for Video Retrieval.**<br>
*M Dzabraev, M Kalashnikov, et al.*<br>
CVPR, 2021. 
[[PDF](https://openaccess.thecvf.com/content/CVPR2021W/HVU/papers/Dzabraev_MDMMT_Multidomain_Multimodal_Transformer_for_Video_Retrieval_CVPRW_2021_paper.pdf)]


